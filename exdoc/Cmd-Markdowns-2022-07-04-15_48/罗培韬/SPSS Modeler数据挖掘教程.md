# SPSS Modeler数据挖掘教程

标签（空格分隔）： 施华 罗培韬

---

[TOC]

# **1.SPSS Modeler介绍**
## **1.1 什么是数据挖掘**
+ 无论是在过去、现在还是未来，人们总是希望能够借助观察事物（获取数据），通过合适的手段（ 建立统计挖掘模型）来量化这些关系。例如，借助一个人的身高来预测他的体重。
+ 在实际的商业实践中，可以将数据挖掘任务简单分为预测任务和控制任务。在预测任务中，我们更加关心得是对目标变量Y得预测；在控制任务中，我们希望能够尽可能地描述清楚X与Y得关系
+ CRISP-DM（Cross Industry Standard Process for Data Mining，跨行业数据挖掘标准流程）它将一个数据挖掘项目划分为6个步骤：商业理解、数据理解、数据准备、建立模型、模型评估和结果部署。
![1.PNG-94.1kB][1]

## **1.2 SPSS Modeler窗口和功能介绍**
![2.PNG-128.4kB][2]
+ SPSS Modeler主要分为四个区域，数据流编辑区、节点工具区、流管理窗口、项目管理窗口
+ 数据流编辑区：建立和编辑数据流的区域，用户的大部分操作是在这个区域内完成的。
+ 节点工具区：工具箱中分类存放着所有节点工具，包括数据采集、数据展示、数据预处理、数据建模以及模型评价，扩展算法等功能。
## **1.3 SPSS Modeler数据流的基本管理和执行**
+ 数据流操作流程：选择和管理节点——节点连接和连接调整——设置节点参数——执行数据流
+ 选择和管理节点：收藏选项卡（存放最常用的节点工具)、数据源选项卡（读取外部数据的节点工具）、记录选项卡（从行的角度处理数据）、字段选项卡（从列的角度处理数据）、图形选项卡（可视化工具）、建模选项卡（各种算法模型）、输出选项卡（展示数据特征的工具箱）、导出选项卡（数据转换工具）
+ 节点连接和连接调整：鼠标右击菜单中有常用的节点之间操作，拖拽即可。
+ 设置节点参数：该部分的参数设置需要理解每一个算法模型背后的基本原理，调参才能正确正常地进行。
+ 执行数据流：直接点击运行按钮或在工具菜单中选择运行。
+ 其他节点和功能：缓冲节点（数据载入节点)、超节点（合并多个节点）、节点映射（将数据挖掘过程和源数据解耦）
## **1.4 SPSS Modeler从一个示例看SPSS Modeler的使用**
+ 流文件：药物研究.str（str后缀都是流文件)
+ 第一步，读入数据（源选项卡-变量文件）
+ 第二步，浏览数据（输出选项卡-表格）
+ 第三步，数据分布（输出选项卡-分析）
+ 第四步，数据可视化（图形选项卡-散点图)
+ 第五步，数据可视化（图形选项卡-直方图）
+ 第六步，数据可视化（图形选项卡-网络图）
+ 第七步，特征工程（字段选项卡-导出）、(字段选项卡-过滤器)
+ 第八步，C5.0建模（模型选项卡-C5.0）、(输出选项卡-分析)
# **2.安装与数据读取**
+ 流文件：读入数据.str
## **2.1 SPSS Modeler的安装与扩展**
+ waston：autoML云端平台，SPSS Modeler之前更关注于统计建模，目前已支持大数据，机器学习。
+ 扩展：支持R/Python for spark的集成，可以直接自定义一个建模节点。（扩展菜单-定制节点对话框构建器）
## **2.2 读入数据**
### **2.2.1 读自由格式的文本文件**
+ 自由格式包括txt,csv等行样本格式数据文件，用源选项卡中的变量文件节点。
### **2.2.2 读EXCEL电子表格数据**
+ 用源选项卡中的EXCEL节点
### **2.2.3 读SPSS格式文件**
+ 用源选项卡中的SPSS统计文件节点
### **2.2.4 读数据库文件**
+ 用源选项卡中的数据库节点
## **2.3 生成实验方案数据**
+ 实验性数据可以用源选项卡中的用户输入节点

# **3.文件数据变量介绍和预处理**
+ 流文件：变量管理.str，样本管理.str
## **3.1 变量说明**
+ 主要用字段选项卡中的类型节点
### **3.1.1 取值范围和缺失值的说明**
+ 缺失值开关
+ 指定范围
### **3.1.2 变量取值有效性和修正**
+ 处理方式：无效、强制、丢弃、警告、中止
### **3.1.3 变量角色的说明**
+ 指定自变量和因变量
## **3.2 变量预处理**
### **3.2.1 变量值的重新计算**
+ CLEM(SPSS Modeler Language for Expression Mainpulation):SPSS专门用于表述运算操作的语言。
+ 主要用字段选项卡中的导出节点
### **3.2.2 变量类别值的调整**
+ 主要用字段选项卡中的重新分类节点
### **3.2.3 生成新变量**
+ 主要用字段选项卡中的导出节点
### **3.2.4 变量值的离散化处理**
+ 离散化处理又叫做分箱操作。常用的分箱方法有组距分组、分位数分组、单变量值分组、均值标准差分组和基于最短描述长度原则的熵分组。
+ 主要用字段选项卡中的分箱节点。
### **3.2.5 生成样本集分割变量**
+ 样本分割主要是划分出训练集、测试集和预测集。常用的方法有Holdout、N-fold交叉验证和重抽样自助法
+ 主要用字段选项卡中的分区节点
## **3.3 样本管理**
### **3.3.1 样本排序**
+ 主要用字段选项卡中的字段重排节点
### **3.3.2 样本的条件筛选**
+ 主要用记录选项卡中的选择节点
### **3.3.3 样本的随机抽样**
+ 主要用记录选项卡中的样本节点
### **3.3.4 样本的浓缩处理**
+ 主要用记录选项卡中的区分节点
### **3.3.5 样本的分类汇总**
+ 主要用记录选项卡中的RFM汇总节点
### **3.3.6 样本的平衡处理**
+ 样本不平衡主要解决方法有过采样和欠采样。
+ 主要用记录选项卡中的平衡节点
### **3.3.7 样本的转置和重新组织**
+ 主要用记录选项卡中的转置节点和重组织节点
### **3.4 数据质量探索与处理**
+ 数据质量探索主要用输出选项卡中的数据审核节点
+ 缺失值处理主要用字段选项卡中的填充节点

# **4.数据可视化操作**
## **4.1 统计图表分析**
+ 主要用图形选项卡中的所有节点

# **5.主成分分析**
## **5.1 主成分分析介绍**
+ 主成分分析的本质就是降维，将高维数据有效地转化为低维数据来处理，揭示变量之间的内在联系，进而分析解决实际问题。
+ 当一个变量只取一个数据时，这个变量提供的信息量非常有限，当这个变量取一系列不同数据时，我们可以从中读出最大值、最小值、平均数等信息。变量的变异性越大，说明它提供的信息量就越大。主成分分析中的信息，就是指变量的变异性，其用标准差或方差表示。
## **5.2 总体主成分**
+ 从协方差矩阵出发，利用矩阵分解（特征值方法）来计算主成分。
+ 主成分个数的确定，用方差贡献率来衡量，通常取使得累积贡献率达到80%的个数。
## **5.3 样本主成分**
+ 总体主成分从总体协方差矩阵出发，其结果通常会受变量单位的影响，为了去量纲，我们用相关系数矩阵来代替协方差矩阵。
## **5.4 使用案例**

# **6.相关性分析和关联分析**
+ 流文件：t检验.str,卡方检验.str,相关分析.str;关联规则.str
## **6.1 相关性分析**
### **6.1.1 分类变量相关性分析**
+ 流文件：卡方检验.str
+ 卡方检验：基于列联表检验两变量间的独立性。
### **6.1.2 连续变量相关性分析**
+ 流文件：相关分析.str
+ 相关系数：常见的相关系数为Pearson相关系数。
### **6.1.3 连续变量与分类变量关系**
+ T检验：独立样本T检验和配对样本T检验
+ 流文件：t检验.str
### **6.1.4 使用案例**
## **6.2 关联分析**
+ 事物之间的关联关系包括简单关系和序列关联关系
### **6.2.1 简单关联规则**
+ 事务和项集：简单关联规则的分析对象是事务。事务可以理解为一种商业行为。
+ 规则置信度和规则支持度：规则置信度是对简单关联规则准确度的测量；规则支持度测度了简单关联规则的普遍性。
### **6.2.2 Apriori算法及应用**
+ SPSS Modeler采用的是Christian Borgelt对Apriori算法的改进算法。
#### **6.2.2.1 产生频繁项集**
+ 设定最小支持度，用来产生频繁项集
#### **6.2.2.2 依据频繁项集产生简单关联规则**
+ 设定最小置信度，用来产生简单关联规则
#### **6.2.2.3 Apriori算法的应用示例**
### **6.2.3 序列关联分析**
#### **6.2.3.1 序列关联介绍**
+ 序列关联研究的最终目标是生成序列关联规则。序列关联规则能够反映事物发展的前后关联关系，可用于推断事物后续的发生可能。
#### **6.2.3.2 Sequence算法**
+ 同样也需要根据支持度产生频繁序列集，再根据置信度产生关联规则。
+ 由于序列关联分析涉及时间问题，因此有必要限定在怎样的时间范围实施的行为或发生的事物，属于同一时间点上或属于另一时间点上。
#### **6.2.3.3 序列列关联的应用示例**

# **7.聚类分析**
+ 流文件：聚类分析.str
## **7.1 聚类分析介绍**
+ 聚类分析能够将一批样本数据，在没有先验知识的前提下，根据数据的诸多特征，按照其在性质上的亲疏程度进行自动分组，且使组内个体的结构特征具有较大相似性，组间个体的特征相似性较小。
## **7.2 K均值聚类**
### **K-means对亲疏程度的测度**
+ 亲疏程度的测度一般有两个角度：第一，数据间的相似程度；第二，数据间的差异程度。衡量相似程度一般可采用相关系数，差异程度一般通过某种距离来测度。K-means聚类方法采用第二个测度角度。
### **K-means聚类过程**
+ 第一步，指定聚类数目K
+ 第二步，确定K个初始类中心
+ 第三步，根据最近原则进行聚类
+ 第四步，重新确定K个类中心
+ 第五步，判断是否已经满足终止条件，如果没有满足，返回第三步，不断反复上述过程，迭代求解。
+ 但是需要注意的一点是，在做聚类之前需要消除变量的数量级差异，需要预先做标准化处理。还有需要对分类变量数值化处理。
### **K-means聚类的应用示例**

# **8.回归分析**
+ 流文件：回归分析.str
## **8.1 回归分析介绍**
## **8.2 线性回归分析**
### **8.2.1 估计模型（OLS）**
+ 线性回归一般用OLS求解，非线性一般需要用MLE求解（涉及数值优化）。
### **8.2.2 模型检验（T检验，F检验）**
+ T检验是对参数显著性的检验，F检验是对整个线性方程的检验。
### **8.2.3 模型评价 （拟合优度检验）**
+ 拟合优度是判断模型的解释能力，这里和过拟合、欠拟合两个概念有所关联。
+ 欠拟合常用的处理办法是加大样本容量
+ 过拟合常用的处理方法是交叉验证和正则化
### **8.2.4 使用案例**

# **9.经典机器学习算法**
+ 流文件：决策树.str，集成算法.str，神经网络.str
## **9.1 决策树**
### **9.1.1 决策树介绍**
+ 决策树顾名思义就是一个用于决策的树，其结构和人类思考方式很接近，所以可解释性很强。
+ 决策树主要核心有树的生成与修剪
### **9.1.2 C5.0算法**
+ 可以给出一个推理规则集
### **9.1.3 使用案例**
## **9.2 集成学习**
+ 多个弱学习器经过算法融合在一起，组成一个强学习器
### **9.2.1 bagging算法**
+ 它是一种借助从原始数据集中重复抽样生成的新训练数据集，再基于不同的训练数据集构建多个学习器的方法。
### **9.2.2 boosting算法**
+ boosting与bagging最大的差别在于权重，boosting给错判的样本更大的抽样权重。
### **9.2.3 随机森林**
+ 随机森林是在bagging基础上加上了随机选择变量这一方式。
### **9.2.4 使用案例**
## **9.3深度学习**
### **9.3.1 神经网络介绍**
+ 模拟神经元网络，结构设计包括层数，神经元个数，激活函数选择，连接方式等
### **9.3.2 BP神经网络算法**
+ BP算法的核心是网络权值的调整，算法过程涉及数值优化。
### **9.3.3 使用案例**
## **9.4 自定义算法扩展**
+ SPSS Modeler支持对R和Python的集成。
+ 集成R和Python时需要安装集成工具。
+ SPSS Modeler支持自定义节点。




  [1]: http://static.zybuluo.com/tulip0216/zjznv6jzsqizmwar3zr01ysg/1.PNG
  [2]: http://static.zybuluo.com/tulip0216/vlt75o44o5j7wb1392qcsqka/2.PNG